# Copyright 2019 The Chromium Authors. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.
"""Utility functions for code coverage."""

import base64
import difflib
import json
import urllib2

from common.findit_http_client import FinditHttpClient
from gae_libs.caches import PickledMemCache
from libs.cache_decorator import Cached
from services.code_coverage import diff_util

# Mapping from metric names to detailed explanations, and one use case is to use
# as the tooltips.
_METRIC_NAME_DETAIL_MAPPING = {
    'line': (
        "Line coverage is the percentage of code lines which have been "
        "executed at least once. Only executable lines within function bodies "
        "are considered to be code lines."),
    'function': (
        "Function coverage is the percentage of functions which have been "
        "executed at least once. A function is considered to be executed if "
        "any of its instantiations are executed."),
    'region': (
        "Region coverage is the percentage of code regions which have been "
        "executed at least once. A code region may span multiple lines (e.g in "
        "a large function body with no control flow). However, it's also "
        "possible for a single line to contain multiple code regions (e.g in "
        "'return x || y &amp;&amp; z')."),
    'branch': (
        "Branch coverage is the percentage of branches from each decision "
        "point is executed at least once."),
    'instruction': (
        "Java instruction coverage is the percentage of the Java byte code "
        "instructions which have been executed at least once."),
}

# List of patchset kinds that are applicable for sharing coverage data between
# patchsets, and the list of possible kinds is defined in:
# https://gerrit-review.googlesource.com/Documentation/json.html
_NON_CONFLICT_CHANGE_KIND = [
    # Conflict-free merge between the new parent and the prior patch set.
    'TRIVIAL_REBASE',
    # Conflict-free change of first (left) parent of a merge commit.
    'MERGE_FIRST_PARENT_UPDATE',
    # No code changed; same tree and same parent tree.
    'NO_CODE_CHANGE',
    # No changes; same commit message, same tree and same parent tree.
    'NO_CHANGE'
]


def GetMetricsBasedOnCoverageTool(coverage_tool):
  """Gets a list of metrics for the given coverage tool.

  Args:
    coverage_tool(str): Name of the coverage tool, such as clang and jacoco.

  Returns:
    A list of dict of following format:
    {'name': clang, 'detail': blala}, where the name is the name of the metric
    and detail is an explanation of what the metric stands for.
  """
  assert coverage_tool in ('clang', 'jacoco'), (
      'Unrecognized coverage tool: %s' % coverage_tool)

  metrics = []
  if coverage_tool == 'clang':
    metrics = ['line', 'function', 'region']
  else:
    metrics = ['line', 'branch', 'instruction']

  return [{
      'name': m,
      'detail': _METRIC_NAME_DETAIL_MAPPING.get(m, '')
  } for m in metrics]


@Cached(PickledMemCache(), namespace='coverage_equivalent_patchsets')
def GetEquivalentPatchsets(host, project, change, patchset):
  """Gets equivalent patchsets that are applicable for sharing coverage data.

  The reason why this is not just the current patchset number is because there
  may have been a succession of "trivial" changes before the current patchset.

  Args:
    host (str): The host name.
    project (str): The project name.
    change (int): The change number.
    patchset (int): The patchset number.

  Returns:
    A list of equivalent patchset numbers in descending order.
  """
  assert isinstance(change, int), 'Change is expected to be an integer'
  assert isinstance(patchset, int), 'Patchset is expected to be an integer'

  change_details = _FetchChangeDetails(host, project, change)
  revisions = change_details['revisions'].values()
  revisions.sort(key=lambda r: r['_number'], reverse=True)
  patchsets = []
  for i, r in enumerate(revisions):
    if i == 0 and change_details['status'] == 'MERGED':
      # Depending on the submit strategy, the last patchset of submitted CLs
      # might be autogenerated and whose kind is labeled as 'REWORK' even though
      # it's actually trivial rebase.
      #
      # This function assumes that the submit strategy is 'Rebase Always' (such
      # as Chromium project), and it may break for projects with other submit
      # strategies, for example: crbug.com/809182.
      #
      # TODO(crbug.com/809182): Make the equivalent patchsets logic generic
      # across all projects. Note that the bug specifically refers to
      # buildbucket, but the same reasonings apply here.
      continue

    current_patchset = r['_number']
    if current_patchset > patchset:
      continue

    patchsets.append(current_patchset)
    if r['kind'] not in _NON_CONFLICT_CHANGE_KIND:
      # If this revision was a non-trivial change, don't consider patchsets
      # prior to it.
      break

  return patchsets


def _FetchChangeDetails(host, project, change):
  """Fetches change detail for a given change.

  Args:
    host (str): The url of the host.
    project (str): The project name.
    change (int): The change number.

  Returns:
    A dict whose format conforms to the ChangeInfo object:
    https://gerrit-review.googlesource.com/Documentation/rest-api-changes.html#change-info
  """
  # Uses the Get Change API to get and parse the details of this change.
  # https://gerrit-review.googlesource.com/Documentation/rest-api-changes.html#get-change.
  template_to_get_change = (
      'https://%s/changes/%s?o=ALL_REVISIONS&o=SKIP_MERGEABLE')
  url = template_to_get_change % (host, _GetChangeId(project, change))
  status_code, response, _ = FinditHttpClient().Get(url)
  if status_code != 200:
    raise RuntimeError(
        'Failed to get change details with status code: %d' % status_code)

  # Remove XSSI magic prefix
  if response.startswith(')]}\''):
    response = response[4:]

  return json.loads(response)


def _GetChangeId(project, change):
  """Gets the change id for a given change.

  Args:
    project (str): The project name.
    change (int): The change number.

  Returns:
    A string representing a change id according to:
    https://gerrit-review.googlesource.com/Documentation/rest-api-changes.html#change-id
  """
  project_quoted = urllib2.quote(project, safe='')
  return '%s~%d' % (project_quoted, change)


def DecompressLineRanges(line_ranges):
  """Decompress the lines ranges data to a flat format.

  For example:
  [
    {
      "count": 1,
      "first": 165, // inclusive
      "last": 166 // inclusive
    }
  ]

  After decompressing, it becomes:
  [
    {
      "line": 165,
      "count": 1
    },
    {
      "line": 166,
      "count": 1
    }
  ]

  Args:
    line_ranges: A list of dict, with format
                 [{"first": int, "last": int, "count": int}, ...], and note that
                 the [first, last] are both inclusive.

  Returns:
    A list of dict, with format: [{"line": int, "count": int}].
  """
  decompressed_lines = []
  for line_range in line_ranges:
    for line_num in range(line_range['first'], line_range['last'] + 1):
      decompressed_lines.append({
          'line': line_num,
          'count': line_range['count'],
      })

  return decompressed_lines


def CompressLines(lines):
  """Compress the lines data to ranges.

  This is a reverse-operation of DecompressLines.

  Args:
    lines: A list of dict, with format: [{"line": int, "count": int}].

  Returns:
    A list of dict, with format: {"first": int, "last": int, "count": int}, and
    note that the [first, last] are both inclusive.
  """
  range_start_index = 0
  line_ranges = []
  for i in xrange(1, len(lines) + 1):
    is_continous_line = (
        i < len(lines) and lines[i]['line'] == lines[i - 1]['line'] + 1)
    has_same_count = (
        i < len(lines) and lines[i]['count'] == lines[i - 1]['count'])
    if is_continous_line and has_same_count:
      continue

    line_ranges.append({
        'first': lines[range_start_index]['line'],
        'last': lines[i - 1]['line'],
        'count': lines[range_start_index]['count'],
    })
    range_start_index = i

  return line_ranges


def RebasePresubmitCoverageDataBetweenPatchsets(
    host, project, change, patchset_src, patchset_dest, coverage_data_src):
  """Gets line-number rebased coverage data for a patchset based on another one.

  This function assumes that the two patchsets are a sequence of trivial-rebase
  or commit-message-edit away, for more details, please see
  |GetEquivalentPatchsets|.

  The code coverage data format is defined at:
  https://chromium.googlesource.com/infra/infra/+/refs/heads/master/appengine/findit/model/proto/code_coverage.proto

  Args:
    host (str): The url of the host.
    project (str): The project name.
    change (int): The change number.
    patchset_src (int): The patchset number to rebase coverage data from.
    patchset_dest (int): The patchset number to rebase coverage data for.
    coverage_data_src (list): A list of File coverage data of |patchset_src|.

  Returns:
    A list of File coverage data of |patchset_dest|.
  """
  change_details = _FetchChangeDetails(host, project, change)
  files_to_rebase = [line_data['path'][2:] for line_data in coverage_data_src]

  # TODO(crbug.com/910289): Parallelize the requests to get file content.
  files_content_src = _FetchFilesContentFromGerrit(
      host, project, change, patchset_src, files_to_rebase, change_details)
  files_content_dest = _FetchFilesContentFromGerrit(
      host, project, change, patchset_dest, files_to_rebase, change_details)

  assert len(files_content_src) == len(files_content_dest)

  coverage_data_dest = []
  for line_data_src, content_src, content_dest in zip(
      coverage_data_src, files_content_src, files_content_dest):
    diff_lines = list(
        difflib.unified_diff(content_src.splitlines(),
                             content_dest.splitlines()))
    mapping = diff_util.generate_line_number_mapping(diff_lines,
                                                     content_src.splitlines(),
                                                     content_dest.splitlines())

    lines_src = DecompressLineRanges(line_data_src['lines'])
    lines_dest = []
    for line in lines_src:
      if line['line'] not in mapping:
        continue

      lines_dest.append({
          'line': mapping[line['line']][0],
          'count': line['count']
      })

    blocks_src = line_data_src.get('uncovered_blocks', [])
    blocks_dest = []
    for block in blocks_src:
      if block['line'] not in mapping:
        continue

      blocks_dest.append({
          'line': mapping[block['line']][0],
          'ranges': block['ranges'],
      })

    line_data_dest = {
        'path': line_data_src['path'],
        'lines': CompressLines(lines_dest),
    }
    if blocks_dest:
      line_data_dest['uncovered_blocks'] = blocks_dest

    coverage_data_dest.append(line_data_dest)

  # TODO(crbug.com/910289): Filter the coverage data by list of files that are
  # actually changed by the patchset.
  return coverage_data_dest


def _FetchFilesContentFromGerrit(host, project, change, patchset, file_paths,
                                 change_details):
  """Fetches file content for a list of files from Gerrit.

  Args:
    host (str): The url of the host.
    project (str): The project name.
    change (int): The change number.
    patchset (int): The patchset number.
    file_paths (list): A list of file paths that are relative to the checkout.
    change_details (dict): The format conforms to the ChangeInfo object:
                           https://gerrit-review.googlesource.com/Documentation/rest-api-changes.html#change-info

  Returns:
    A list of String where each one corresponds to the content of each file.
  """
  patchset_revision = None
  for revision, value in change_details['revisions'].iteritems():
    if patchset == value['_number']:
      patchset_revision = revision
      break

  if not patchset_revision:
    raise RuntimeError(
        'Patchset %d is not found in the returned change details: %s' %
        (patchset, json.dumps(change_details)))

  result = []
  # TODO(crbug.com/910289): Parallelize the requests to get file content.
  for file_path in file_paths:
    # Uses the Get Content API to get the file content from Gerrit.
    # https://gerrit-review.googlesource.com/Documentation/rest-api-changes.html#get-content
    quoted_file_path = urllib2.quote(file_path, safe='')
    url = ('https://%s/changes/%s/revisions/%s/files/%s/content' %
           (host, _GetChangeId(project, change), patchset_revision,
            quoted_file_path))

    status_code, response, _ = FinditHttpClient().Get(url)
    if status_code != 200:
      raise RuntimeError(
          'Failed to get change details with status code: %d' % status_code)

    content = base64.b64decode(response)
    result.append(content)

  return result
