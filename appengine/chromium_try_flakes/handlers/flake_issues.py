# Copyright 2015 The Chromium Authors. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

"""Task queue endpoints for creating and updating issues on issue tracker."""

import datetime
import json
import logging
import random
import sha
import webapp2

from google.appengine.api import taskqueue
from google.appengine.api import urlfetch
from google.appengine.ext import ndb

from issue_tracker.issue_tracker_api import IssueTrackerAPI
from issue_tracker.issue import Issue
from model.flake import FlakeUpdateSingleton, FlakeUpdate


FLAKY_RUNS_TEMPLATE = (
    'Detected new flakes for test/step "%(name)s":\n\n%(flaky_runs)s\n'
    'This message was automatically generated by the chromium-try-flakes app.')
FLAKY_RUN_TEMPLATE = (
    '  Failure: %(failure_url)s.\n  Success: %(success_url)s.\n')
SUMMARY_TEMPLATE = 'Test/step "%(name)s" is flaky'
DESCRIPTION_TEMPLATE = (
    '%(summary)s.\n\n'
    'This issue was created automatically by the chromium-try-flakes app and '
    'was assigned to the current sheriff. Please find the right owner to fix '
    'the respective test/step and re-assign this issue to them. If the '
    'step/test is infrastructure-related, please add a label Infra=Troopers, '
    'remove yourself as an owner and mark the issue as Untriaged.')
REOPENED_DESCRIPTION_TEMPLATE = (
    '%(description)s\n\n'
    'This flaky test/step was previously tracked in issue %(old_issue)d.')
MAX_UPDATED_ISSUES_PER_DAY = 50


@ndb.transactional
def _get_flake_update_singleton_key():
  """Returns a key for the FlakeUpdateSingleton singleton entity."""
  singleton_key = ndb.Key('FlakeUpdateSingleton', 'singleton')
  if not singleton_key.get():
    FlakeUpdateSingleton(key=singleton_key).put()
  return singleton_key


class UpdateIssue(webapp2.RequestHandler):
  @ndb.non_transactional
  def _get_flaky_runs(self, flake):
    # Only report up to 20 last runs.
    num_runs = min(len(flake.occurrences) - flake.num_reported_flaky_runs, 20)
    return ndb.get_multi(flake.occurrences[-num_runs:])

  @ndb.non_transactional
  def _format_flaky_runs_msg(self, test_name, new_flaky_runs):
    flaky_run_msg_parts = []
    for run in new_flaky_runs:
      flaky_run_msg_parts.append(
          FLAKY_RUN_TEMPLATE % {'failure_url': run.failure_run.get().getURL(),
                                'success_url': run.success_run.get().getURL()})
    return FLAKY_RUNS_TEMPLATE % {'flaky_runs': '\n'.join(flaky_run_msg_parts),
                                  'name': test_name}

  def recreate_issue_for_flake(self, flake):
    """Updates a flake to re-create an issue and creates a respective task."""
    flake.old_issue_id = flake.issue_id
    flake.issue_id = 0
    flake.put()
    taskqueue.add(url='/issues/process/%s' % flake.key.urlsafe(),
                  queue_name='issue-updates', transactional=True)

  @ndb.transactional
  def post(self, urlsafe_key):
    """Updates an issue on the issue tracker."""
    now = datetime.datetime.utcnow()
    flake = ndb.Key(urlsafe=urlsafe_key).get()

    # Retrieve flaky runs outside of the transaction, because we are not
    # planning to modify them and because there could be more of them than the
    # number of groups supported by cross-group transactions on AppEngine.
    new_flaky_runs = self._get_flaky_runs(flake)
    flake.num_reported_flaky_runs = len(flake.occurrences)

    api = IssueTrackerAPI('chromium')
    issue = api.getIssue(flake.issue_id)

    # Handle cases when an issue has been closed. We need to do this in a loop
    # because we might move onto another issue.
    seen_issues = set()
    while not issue.open:
      if issue.status == 'Duplicate':
        # If the issue was marked as duplicate, we update the issue ID stored in
        # datastore to the one it was merged into and continue working with the
        # new issue.
        seen_issues.add(issue.id)
        if issue.merged_into not in seen_issues:
          flake.issue_id = issue.merged_into
          issue = api.getIssue(flake.issue_id)
        else:
          logging.info('Detected issue duplication loop: %s. Re-creating an '
                       'issue for the flake %s.', seen_issues, flake.name)
          self.recreate_issue_for_flake(flake)
          return
      else:  # Fixed, WontFix, Verified, Archived, custom status
        # If the issue was closed, we do not update it. This allows changes made
        # to reduce flakiness to propagate and take effect. If after one week we
        # still see flakiness, we will create a new issue.
        if issue.updated < now - datetime.timedelta(weeks=1):
          self.recreate_issue_for_flake(flake)
        return

    new_flaky_runs_msg = self._format_flaky_runs_msg(flake.name, new_flaky_runs)
    api.update(issue, comment=new_flaky_runs_msg)
    logging.info('Updated issue %d for flake %s with %d flake runs',
                 flake.issue_id, flake.name, len(new_flaky_runs))
    flake.issue_last_updated = now

    # Note that if transaction fails for some reason at this point, we may post
    # updates multiple times. On the other hand, this should be extremely rare
    # becase we set the number of concurrently running tasks to 1, therefore
    # there should be no contention for updating this issue's entity.
    flake.put()


class CreateIssue(webapp2.RequestHandler):
  @ndb.transactional
  def post(self, urlsafe_key):
    flake = ndb.Key(urlsafe=urlsafe_key).get()

    summary = SUMMARY_TEMPLATE % {'name': flake.name}
    description = DESCRIPTION_TEMPLATE % {'summary': summary}
    if flake.old_issue_id:
      description = REOPENED_DESCRIPTION_TEMPLATE % {
          'description': description, 'old_issue': flake.old_issue_id}

    api = IssueTrackerAPI('chromium')
    issue = Issue({'summary': summary,
                   'description': description,
                   'status': 'Untriaged',
                   'labels': ['Type-Bug', 'Pri-1', 'Cr-Tests-Flaky',
                              'Via-TryFlakes', 'Sheriff-Chromium']})
    flake.issue_id = api.create(issue).id

    logging.info('Created a new issue %d for flake %s', flake.issue_id,
                 flake.name)
    flake.put()

    # Process the issue again after this transaction is committed to post
    # updates about the latest flaky runs.
    taskqueue.add(url='/issues/process/%s' % flake.key.urlsafe(),
                  queue_name='issue-updates', transactional=True)


class ProcessIssue(webapp2.RequestHandler):
  def _increment_update_counter(self):
    # Optimistically record the fact that we've updated issue tracker. We cannot
    # do this update post-factum because we would then be able to schedule too
    # many tasks updating issue tracker before incrementing the counter and then
    # still file too many issues.
    FlakeUpdate(parent=_get_flake_update_singleton_key()).put()

  @ndb.transactional(xg=True)  # pylint: disable=E1120
  def post(self, urlsafe_key):
    # Check if we should stop processing this issue because we've posted too
    # many updates to issue tracker today already.
    day_ago = datetime.datetime.utcnow() - datetime.timedelta(days=1)
    num_updates_last_day = FlakeUpdate.query(
        FlakeUpdate.time_updated > day_ago,
        ancestor=_get_flake_update_singleton_key()).count()
    if num_updates_last_day >= MAX_UPDATED_ISSUES_PER_DAY:
      return

    now = datetime.datetime.utcnow()
    flake = ndb.Key(urlsafe=urlsafe_key).get()

    if flake.issue_id > 0:
      # Update issues at most once a day.
      if flake.issue_last_updated > now - datetime.timedelta(days=1):
        return

      # Only update issues if there are new flaky runs.
      if flake.num_reported_flaky_runs == len(flake.occurrences):
        return

      # We schedule this task transactionally because we want to ensure that we
      # have checked Flake properties and the limit of daily updates and there
      # was no contention with another /issues/process task.
      taskqueue.add(url='/issues/update/%s' % flake.key.urlsafe(),
                    queue_name='issue-updates', transactional=True)
      self._increment_update_counter()
    else:
      # We reduce the likely hood of creating multiple issues for the same flake
      # by using named tasks. If two processes schedule two tasks with the same
      # name, only one will be executed. We also append previous issue ID to the
      # name to allow re-creating issues, e.g. when a previous issue was clsoed.
      task_id = sha.new(flake.name).hexdigest()  # sanitize name for task
      task_name = 'create_issue_%s_%s' % (task_id, flake.old_issue_id)
      try:
        taskqueue.add(name=task_name, queue_name='issue-updates',
                      url='/issues/create/%s' % flake.key.urlsafe())
        self._increment_update_counter()
      except taskqueue.TombstonedTaskError:
        pass
